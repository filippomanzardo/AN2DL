# We use __<something>__ to put placeholders that will be replaced
import tensorflow as tf
import zipfile
import tempfile
from datetime import datetime
import os
from google.cloud import storage
import tensorflow_cloud as tfc
from typing import Any

__MODEL_FUNCTION_HERE__

__PREPROCESS_HERE__

GCP_BUCKET = "polimi-training"
CHECKPOINT_PATH = os.path.join("gs://", GCP_BUCKET, "challenge_1", "__NET_NAME___save_at_{epoch}_", datetime.now().strftime("%Y%m%d-%H%M%S"))
TENSORBOARD_PATH = os.path.join(
    "gs://", GCP_BUCKET, "logs", datetime.now().strftime("%Y%m%d-%H%M%S")
)
CALLBACKS = [
    tf.keras.callbacks.TensorBoard(log_dir=TENSORBOARD_PATH, histogram_freq=1),
]
FINE_TUNING_CALLBACK = [tf.keras.callbacks.TensorBoard(log_dir=TENSORBOARD_PATH + "_tuned", histogram_freq=1)]

client = storage.Client()
bucket = client.get_bucket(GCP_BUCKET)
dataset = bucket.get_blob("challenge_1/dataset.zip")

dataset_directory = "./dataset"
with tempfile.TemporaryDirectory() as temp_dir:
    dataset.download_to_filename(os.path.join(temp_dir, "dataset.zip"))
    os.makedirs(dataset_directory, exist_ok=True)

    with zipfile.ZipFile(os.path.join(temp_dir, "dataset.zip"), "r") as zip_ref:
        zip_ref.extractall(dataset_directory)


generator = tf.keras.preprocessing.image.ImageDataGenerator(
    rotation_range=20,
    height_shift_range=0.3,
    width_shift_range=0.4,
    zoom_range=0.4,
    horizontal_flip=True,
    vertical_flip=True,
    brightness_range=[0.3,1.4],
    fill_mode='nearest'
)

training_dataset = generator.flow_from_directory(
    directory=dataset_directory,
    target_size=(96, 96),
    color_mode="rgb",
)

class_list = training_dataset.classes.tolist()
n_class = [class_list.count(i) for i in training_dataset.class_indices.values()]

class_weight = {
    idx: n_class[idx] / sum(n_class) for idx, class_appearances in enumerate(n_class)
}

model = get_model()
model.compile(
    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),
    loss=tf.keras.losses.CategoricalCrossentropy(),
    metrics=["accuracy", tf.metrics.Precision(), tf.metrics.Recall()],
)

if tfc.remote():
    epochs = __EPOCHS__
    batch_size = 16
else:
    epochs = __EPOCHS__
    batch_size = 128

model.fit(preprocess(training_dataset), epochs=epochs, callbacks=CALLBACKS, batch_size=batch_size, class_weight=class_weight)

save_path = os.path.join("gs://", GCP_BUCKET, "__NET_NAME___" + datetime.now().strftime("%Y%m%d_%H%M%S"))

if tfc.remote():
    model.save(save_path)

if __FINE_TUNING__:
    model.trainable = False
    fine_tune_from = -50 if len(model.layers) > 100 else -25

    for layer in model.layers[fine_tune_from:]:
        if not isinstance(layer, tf.keras.layers.BatchNormalization):
            layer.trainable = True

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss=tf.keras.losses.CategoricalCrossentropy(),
        metrics=["accuracy", tf.metrics.Precision(), tf.metrics.Recall()],
    )

    model.fit(preprocess(training_dataset), epochs=epochs, callbacks=FINE_TUNING_CALLBACK, batch_size=batch_size, class_weight=class_weight)

    save_path = os.path.join("gs://", GCP_BUCKET, "__NET_NAME___" + datetime.now().strftime("%Y%m%d_%H%M%S") + "_fine_tuned")

    if tfc.remote():
        model.save(save_path)



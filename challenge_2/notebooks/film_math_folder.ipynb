{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from os import path\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow.keras as keras\n",
    "import pickle\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from utils.callbacks import SaveBestModelInMemory\n",
    "from utils.submission import create_submission_zip"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Constants"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "NUM_CLASSES = 12\n",
    "RANDOM_STATE = 42 # Seed for rng to make everything reproducible and deterministic af\n",
    "SAVED_MODELS_PATH = \"saved-models\"\n",
    "TENSORBOARD_LOGS_PATH = \"tensorboard-logs\"\n",
    "SUBMISSIONS_PATH = \"../submissions\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256 # Number of samples in a mini batch\n",
    "EPOCHS = 200 # Number of training epochs before the training is stopped\n",
    "TEST_SPLIT = 0.15 # Percent of data to use for validation/testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Loading and Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All samples shape: (2429, 36, 6), all labels shape: (2429,)\n",
      "Train samples shape: (2064, 36, 6), Train labels shape: (2064,)\n",
      "Test samples shape: (365, 36, 6), Test labels shape: (365,)\n"
     ]
    }
   ],
   "source": [
    "data = np.load(file=\"../dataset/x_train.npy\")\n",
    "labels = np.load(file=\"../dataset/y_train.npy\")\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=TEST_SPLIT, random_state=RANDOM_STATE)\n",
    "\n",
    "for feature in range(train_data.shape[-1]):\n",
    "    scaler = RobustScaler()\n",
    "    train_data[:,:,feature] = scaler.fit_transform(train_data[:,:,feature])\n",
    "    test_data[:,:,feature] = scaler.transform(test_data[:,:,feature])\n",
    "# Make sure everything was loaded correctly:\n",
    "print(f\"All samples shape: {data.shape}, all labels shape: {labels.shape}\")\n",
    "print(f\"Train samples shape: {train_data.shape}, Train labels shape: {train_labels.shape}\")\n",
    "print(f\"Test samples shape: {test_data.shape}, Test labels shape: {test_labels.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Definition"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import Any, Optional\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "_EPSILON = tf.keras.backend.epsilon()\n",
    "\n",
    "\n",
    "def sparse_categorical_focal_loss(y_true, y_pred, gamma, *,\n",
    "                                  class_weight: Optional[Any] = None,\n",
    "                                  from_logits: bool = False, axis: int = -1\n",
    "                                  ) -> tf.Tensor:\n",
    "    r\"\"\"Focal loss function for multiclass classification with integer labels.\n",
    "    This loss function generalizes multiclass softmax cross-entropy by\n",
    "    introducing a hyperparameter called the *focusing parameter* that allows\n",
    "    hard-to-classify examples to be penalized more heavily relative to\n",
    "    easy-to-classify examples.\n",
    "    See :meth:`~focal_loss.binary_focal_loss` for a description of the focal\n",
    "    loss in the binary setting, as presented in the original work [1]_.\n",
    "    In the multiclass setting, with integer labels :math:`y`, focal loss is\n",
    "    defined as\n",
    "    .. math::\n",
    "        L(y, \\hat{\\mathbf{p}})\n",
    "        = -\\left(1 - \\hat{p}_y\\right)^\\gamma \\log(\\hat{p}_y)\n",
    "    where\n",
    "    *   :math:`y \\in \\{0, \\ldots, K - 1\\}` is an integer class label (:math:`K`\n",
    "        denotes the number of classes),\n",
    "    *   :math:`\\hat{\\mathbf{p}} = (\\hat{p}_0, \\ldots, \\hat{p}_{K-1})\n",
    "        \\in [0, 1]^K` is a vector representing an estimated probability\n",
    "        distribution over the :math:`K` classes,\n",
    "    *   :math:`\\gamma` (gamma, not :math:`y`) is the *focusing parameter* that\n",
    "        specifies how much higher-confidence correct predictions contribute to\n",
    "        the overall loss (the higher the :math:`\\gamma`, the higher the rate at\n",
    "        which easy-to-classify examples are down-weighted).\n",
    "    The usual multiclass softmax cross-entropy loss is recovered by setting\n",
    "    :math:`\\gamma = 0`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : tensor-like\n",
    "        Integer class labels.\n",
    "    y_pred : tensor-like\n",
    "        Either probabilities or logits, depending on the `from_logits`\n",
    "        parameter.\n",
    "    gamma : float or tensor-like of shape (K,)\n",
    "        The focusing parameter :math:`\\gamma`. Higher values of `gamma` make\n",
    "        easy-to-classify examples contribute less to the loss relative to\n",
    "        hard-to-classify examples. Must be non-negative. This can be a\n",
    "        one-dimensional tensor, in which case it specifies a focusing parameter\n",
    "        for each class.\n",
    "    class_weight: tensor-like of shape (K,)\n",
    "        Weighting factor for each of the :math:`k` classes. If not specified,\n",
    "        then all classes are weighted equally.\n",
    "    from_logits : bool, optional\n",
    "        Whether `y_pred` contains logits or probabilities.\n",
    "    axis : int, optional\n",
    "        Channel axis in the `y_pred` tensor.\n",
    "    Returns\n",
    "    -------\n",
    "    :class:`tf.Tensor`\n",
    "        The focal loss for each example.\n",
    "    Examples\n",
    "    --------\n",
    "    This function computes the per-example focal loss between a one-dimensional\n",
    "    integer label vector and a two-dimensional prediction matrix:\n",
    "    >>> import numpy as np\n",
    "    >>> from focal_loss import sparse_categorical_focal_loss\n",
    "    >>> y_true = [0, 1, 2]\n",
    "    >>> y_pred = [[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.2, 0.2, 0.6]]\n",
    "    >>> loss = sparse_categorical_focal_loss(y_true, y_pred, gamma=2)\n",
    "    >>> np.set_printoptions(precision=3)\n",
    "    >>> print(loss.numpy())\n",
    "    [0.009 0.032 0.082]\n",
    "    Warnings\n",
    "    --------\n",
    "    This function does not reduce its output to a scalar, so it cannot be passed\n",
    "    to :meth:`tf.keras.Model.compile` as a `loss` argument. Instead, use the\n",
    "    wrapper class :class:`~focal_loss.SparseCategoricalFocalLoss`.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] T. Lin, P. Goyal, R. Girshick, K. He and P. Doll√°r. Focal loss for\n",
    "        dense object detection. IEEE Transactions on Pattern Analysis and\n",
    "        Machine Intelligence, 2018.\n",
    "        (`DOI <https://doi.org/10.1109/TPAMI.2018.2858826>`__)\n",
    "        (`arXiv preprint <https://arxiv.org/abs/1708.02002>`__)\n",
    "    See Also\n",
    "    --------\n",
    "    :meth:`~focal_loss.SparseCategoricalFocalLoss`\n",
    "        A wrapper around this function that makes it a\n",
    "        :class:`tf.keras.losses.Loss`.\n",
    "    \"\"\"\n",
    "    # Process focusing parameter\n",
    "    gamma = tf.convert_to_tensor(gamma, dtype=tf.dtypes.float32)\n",
    "    gamma_rank = gamma.shape.rank\n",
    "    scalar_gamma = gamma_rank == 0\n",
    "\n",
    "    # Process class weight\n",
    "    if class_weight is not None:\n",
    "        class_weight = tf.convert_to_tensor(class_weight,\n",
    "                                            dtype=tf.dtypes.float32)\n",
    "\n",
    "    # Process prediction tensor\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_pred_rank = y_pred.shape.rank\n",
    "    if y_pred_rank is not None:\n",
    "        axis %= y_pred_rank\n",
    "        if axis != y_pred_rank - 1:\n",
    "            # Put channel axis last for sparse_softmax_cross_entropy_with_logits\n",
    "            perm = list(itertools.chain(range(axis),\n",
    "                                        range(axis + 1, y_pred_rank), [axis]))\n",
    "            y_pred = tf.transpose(y_pred, perm=perm)\n",
    "    elif axis != -1:\n",
    "        raise ValueError(\n",
    "            f'Cannot compute sparse categorical focal loss with axis={axis} on '\n",
    "            'a prediction tensor with statically unknown rank.')\n",
    "    y_pred_shape = tf.shape(y_pred)\n",
    "\n",
    "    # Process ground truth tensor\n",
    "    y_true = tf.dtypes.cast(y_true, dtype=tf.dtypes.int64)\n",
    "    y_true_rank = y_true.shape.rank\n",
    "\n",
    "    if y_true_rank is None:\n",
    "        raise NotImplementedError('Sparse categorical focal loss not supported '\n",
    "                                  'for target/label tensors of unknown rank')\n",
    "\n",
    "    reshape_needed = (y_true_rank is not None and y_pred_rank is not None and\n",
    "                      y_pred_rank != y_true_rank + 1)\n",
    "    if reshape_needed:\n",
    "        y_true = tf.reshape(y_true, [-1])\n",
    "        y_pred = tf.reshape(y_pred, [-1, y_pred_shape[-1]])\n",
    "\n",
    "    if from_logits:\n",
    "        logits = y_pred\n",
    "        probs = tf.nn.softmax(y_pred, axis=-1)\n",
    "    else:\n",
    "        probs = y_pred\n",
    "        logits = tf.math.log(tf.clip_by_value(y_pred, _EPSILON, 1 - _EPSILON))\n",
    "\n",
    "    xent_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y_true,\n",
    "        logits=logits,\n",
    "    )\n",
    "\n",
    "    y_true_rank = y_true.shape.rank\n",
    "    probs = tf.gather(probs, y_true, axis=-1, batch_dims=y_true_rank)\n",
    "    if not scalar_gamma:\n",
    "        gamma = tf.gather(gamma, y_true, axis=0, batch_dims=y_true_rank)\n",
    "    focal_modulation = (1 - probs) ** gamma\n",
    "    loss = focal_modulation * xent_loss\n",
    "\n",
    "    if class_weight is not None:\n",
    "        class_weight = tf.gather(class_weight, y_true, axis=0,\n",
    "                                 batch_dims=y_true_rank)\n",
    "        loss *= class_weight\n",
    "\n",
    "    if reshape_needed:\n",
    "        loss = tf.reshape(loss, y_pred_shape[:-1])\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class SparseCategoricalFocalLoss(tf.keras.losses.Loss):\n",
    "    r\"\"\"Focal loss function for multiclass classification with integer labels.\n",
    "    This loss function generalizes multiclass softmax cross-entropy by\n",
    "    introducing a hyperparameter :math:`\\gamma` (gamma), called the\n",
    "    *focusing parameter*, that allows hard-to-classify examples to be penalized\n",
    "    more heavily relative to easy-to-classify examples.\n",
    "    This class is a wrapper around\n",
    "    :class:`~focal_loss.sparse_categorical_focal_loss`. See the documentation\n",
    "    there for details about this loss function.\n",
    "    Parameters\n",
    "    ----------\n",
    "    gamma : float or tensor-like of shape (K,)\n",
    "        The focusing parameter :math:`\\gamma`. Higher values of `gamma` make\n",
    "        easy-to-classify examples contribute less to the loss relative to\n",
    "        hard-to-classify examples. Must be non-negative. This can be a\n",
    "        one-dimensional tensor, in which case it specifies a focusing parameter\n",
    "        for each class.\n",
    "    class_weight: tensor-like of shape (K,)\n",
    "        Weighting factor for each of the :math:`k` classes. If not specified,\n",
    "        then all classes are weighted equally.\n",
    "    from_logits : bool, optional\n",
    "        Whether model prediction will be logits or probabilities.\n",
    "    **kwargs : keyword arguments\n",
    "        Other keyword arguments for :class:`tf.keras.losses.Loss` (e.g., `name`\n",
    "        or `reduction`).\n",
    "    Examples\n",
    "    --------\n",
    "    An instance of this class is a callable that takes a rank-one tensor of\n",
    "    integer class labels `y_true` and a tensor of model predictions `y_pred` and\n",
    "    returns a scalar tensor obtained by reducing the per-example focal loss (the\n",
    "    default reduction is a batch-wise average).\n",
    "    >>> from focal_loss import SparseCategoricalFocalLoss\n",
    "    >>> loss_func = SparseCategoricalFocalLoss(gamma=2)\n",
    "    >>> y_true = [0, 1, 2]\n",
    "    >>> y_pred = [[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.2, 0.2, 0.6]]\n",
    "    >>> loss_func(y_true, y_pred)\n",
    "    <tf.Tensor: shape=(), dtype=float32, numpy=0.040919524>\n",
    "    Use this class in the :mod:`tf.keras` API like any other multiclass\n",
    "    classification loss function class that accepts integer labels found in\n",
    "    :mod:`tf.keras.losses` (e.g.,\n",
    "    :class:`tf.keras.losses.SparseCategoricalCrossentropy`:\n",
    "    .. code-block:: python\n",
    "        # Typical usage\n",
    "        model = tf.keras.Model(...)\n",
    "        model.compile(\n",
    "            optimizer=...,\n",
    "            loss=SparseCategoricalFocalLoss(gamma=2),  # Used here like a tf.keras loss\n",
    "            metrics=...,\n",
    "        )\n",
    "        history = model.fit(...)\n",
    "    See Also\n",
    "    --------\n",
    "    :meth:`~focal_loss.sparse_categorical_focal_loss`\n",
    "        The function that performs the focal loss computation, taking a label\n",
    "        tensor and a prediction tensor and outputting a loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma, class_weight: Optional[Any] = None,\n",
    "                 from_logits: bool = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.gamma = gamma\n",
    "        self.class_weight = class_weight\n",
    "        self.from_logits = from_logits\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Returns the config of the layer.\n",
    "        A layer config is a Python dictionary containing the configuration of a\n",
    "        layer. The same layer can be re-instantiated later (without its trained\n",
    "        weights) from this configuration.\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            This layer's config.\n",
    "        \"\"\"\n",
    "        config = super().get_config()\n",
    "        config.update(gamma=self.gamma, class_weight=self.class_weight,\n",
    "                      from_logits=self.from_logits)\n",
    "        return config\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        \"\"\"Compute the per-example focal loss.\n",
    "        This method simply calls\n",
    "        :meth:`~focal_loss.sparse_categorical_focal_loss` with the appropriate\n",
    "        arguments.\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : tensor-like, shape (N,)\n",
    "            Integer class labels.\n",
    "        y_pred : tensor-like, shape (N, K)\n",
    "            Either probabilities or logits, depending on the `from_logits`\n",
    "            parameter.\n",
    "        Returns\n",
    "        -------\n",
    "        :class:`tf.Tensor`\n",
    "            The per-example focal loss. Reduction to a scalar is handled by\n",
    "            this layer's\n",
    "            :meth:`~focal_loss.SparseCateogiricalFocalLoss.__call__` method.\n",
    "        \"\"\"\n",
    "        return sparse_categorical_focal_loss(y_true=y_true, y_pred=y_pred,\n",
    "                                             class_weight=self.class_weight,\n",
    "                                             gamma=self.gamma,\n",
    "                                             from_logits=self.from_logits)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def compute_weights(labels):\n",
    "    n_classes = Counter(labels)\n",
    "    weights = {}\n",
    "    for class_, n in n_classes.items():\n",
    "        weight = 1 + (1 - n / np.max(n_classes))\n",
    "        weights[class_] = weight\n",
    "from utils.attention import FilmAttention\n",
    "def build_model_1(input_shape: tuple[int,int,int], nb_classes: int) -> tf.keras.Model:\n",
    "        input_layer = keras.layers.Input(input_shape)\n",
    "        drop_first_column = keras.layers.Lambda(lambda x: x[:, :, 1:])(input_layer)\n",
    "\n",
    "        data_aug = tf.keras.Sequential([\n",
    "            tf.keras.layers.GaussianNoise(0.5, seed=RANDOM_STATE),\n",
    "            tf.keras.layers.GaussianDropout(0.5, seed=RANDOM_STATE),\n",
    "        ])(drop_first_column)\n",
    "\n",
    "        concat = keras.layers.Concatenate(axis=-1)([drop_first_column, data_aug])\n",
    "\n",
    "        # conv block -1\n",
    "        conv_x = keras.layers.Conv1D(filters=32, kernel_size=8, padding='same',\n",
    "    )(concat)\n",
    "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "        conv_y = keras.layers.Conv1D(filters=32, kernel_size=5, padding='same')(conv_x)\n",
    "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "        conv_z = keras.layers.Conv1D(filters=32, kernel_size=3, padding='same')(conv_y)\n",
    "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "        # expand channels for the sum\n",
    "        shortcut_y = keras.layers.Conv1D(filters=32, kernel_size=1, padding='same',\n",
    "    )(drop_first_column)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "        output_block_1 = keras.layers.add([shortcut_y, conv_z])\n",
    "        output_block_1 = keras.layers.Activation('relu')(output_block_1)\n",
    "        # conv block -2\n",
    "        conv2 = keras.layers.Conv1D(filters=32,kernel_size=11,strides=1,padding='same',\n",
    "    )(output_block_1)\n",
    "        conv2 = tfa.layers.InstanceNormalization()(conv2)\n",
    "        conv2 = keras.layers.PReLU(shared_axes=[1])(conv2)\n",
    "        conv2 = keras.layers.Dropout(rate=0.3)(conv2)\n",
    "        conv2 = keras.layers.MaxPooling1D(pool_size=2)(conv2)\n",
    "        # conv block -3\n",
    "        conv3 = keras.layers.Conv1D(filters=128,kernel_size=21,strides=1,padding='same',\n",
    "    )(conv2)\n",
    "        conv3 = tfa.layers.InstanceNormalization()(conv3)\n",
    "        conv3 = keras.layers.PReLU(shared_axes=[1])(conv3)\n",
    "        conv3 = keras.layers.Dropout(rate=0.4)(conv3)\n",
    "        # expand channels for the sum\n",
    "        # split for attention\n",
    "        attention_data = keras.layers.Lambda(lambda x: x[:,:,:64])(conv3)\n",
    "        attention_softmax = keras.layers.Lambda(lambda x: x[:,:,64:])(conv3)\n",
    "        # attention mechanism\n",
    "        attention_softmax = keras.layers.Softmax()(attention_softmax)\n",
    "        multiply_layer = keras.layers.Multiply()([attention_softmax,attention_data])\n",
    "        attention_layer = FilmAttention(units=16)(multiply_layer)\n",
    "        # last layer\n",
    "        dense_layer = keras.layers.Dense(units=8,\n",
    "    )(attention_layer)\n",
    "        act_layer = keras.layers.PReLU()(dense_layer)\n",
    "        dropout_layer = keras.layers.Dropout(rate=0.4)(act_layer)\n",
    "        # output layer\n",
    "        output_layer = keras.layers.Dense(units=nb_classes,activation='softmax')(dropout_layer)\n",
    "\n",
    "        return keras.models.Model(inputs=input_layer, outputs=output_layer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 36, 6)]      0           []                               \n",
      "                                                                                                  \n",
      " lambda_4 (Lambda)              (None, 36, 5)        0           ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " sequential_2 (Sequential)      (None, 36, 5)        0           ['lambda_4[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 36, 10)       0           ['lambda_4[0][0]',               \n",
      "                                                                  'sequential_2[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)             (None, 36, 32)       2592        ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 36, 32)      128         ['conv1d_12[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 36, 32)       0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)             (None, 36, 32)       5152        ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 36, 32)      128         ['conv1d_13[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 36, 32)       0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)             (None, 36, 32)       192         ['lambda_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)             (None, 36, 32)       3104        ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 36, 32)      128         ['conv1d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 36, 32)      128         ['conv1d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 36, 32)       0           ['batch_normalization_11[0][0]', \n",
      "                                                                  'batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 36, 32)       0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)             (None, 36, 32)       11296       ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " instance_normalization_4 (Inst  (None, 36, 32)      64          ['conv1d_16[0][0]']              \n",
      " anceNormalization)                                                                               \n",
      "                                                                                                  \n",
      " p_re_lu_6 (PReLU)              (None, 36, 32)       32          ['instance_normalization_4[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 36, 32)       0           ['p_re_lu_6[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 18, 32)      0           ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)             (None, 18, 128)      86144       ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " instance_normalization_5 (Inst  (None, 18, 128)     256         ['conv1d_17[0][0]']              \n",
      " anceNormalization)                                                                               \n",
      "                                                                                                  \n",
      " p_re_lu_7 (PReLU)              (None, 18, 128)      128         ['instance_normalization_5[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 18, 128)      0           ['p_re_lu_7[0][0]']              \n",
      "                                                                                                  \n",
      " lambda_6 (Lambda)              (None, 18, 64)       0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " softmax_2 (Softmax)            (None, 18, 64)       0           ['lambda_6[0][0]']               \n",
      "                                                                                                  \n",
      " lambda_5 (Lambda)              (None, 18, 64)       0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " multiply_2 (Multiply)          (None, 18, 64)       0           ['softmax_2[0][0]',              \n",
      "                                                                  'lambda_5[0][0]']               \n",
      "                                                                                                  \n",
      " film_attention_2 (FilmAttentio  (None, 16)          6144        ['multiply_2[0][0]']             \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 8)            136         ['film_attention_2[0][0]']       \n",
      "                                                                                                  \n",
      " p_re_lu_8 (PReLU)              (None, 8)            8           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 8)            0           ['p_re_lu_8[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 12)           108         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 115,868\n",
      "Trainable params: 115,612\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n",
      "Run tensorboard in a separate process with:\n",
      "tensorboard --logdir /Users/filippomanzardo/repositories/AN2DL/challenge_2/notebooks/tensorboard-logs\n",
      "or\n",
      "tensorboard --logdir /Users/filippomanzardo/repositories/AN2DL/challenge_2/notebooks/tensorboard-logs/film-shit/2022-12-17-18-48-29\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x2d2b4e3b0>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = train_data.shape[1:]\n",
    "classes = NUM_CLASSES\n",
    "model_name = \"film-shit\" # Give your model an awesome name for a 2% percent accuracy increase.\n",
    "\n",
    "model = build_model_1(input_shape, classes)\n",
    "model.compile(loss=SparseCategoricalFocalLoss(gamma=2, class_weight=list(compute_weights(train_labels).values())), optimizer=keras.optimizers.Adam(learning_rate=3e-3), metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "run_id = datetime.utcnow().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "current_tensorboard_log_dir = f\"{TENSORBOARD_LOGS_PATH}/{model_name}/{run_id}\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=current_tensorboard_log_dir)\n",
    "print(f\"Run tensorboard in a separate process with:\\n\"\n",
    "      f\"tensorboard --logdir {path.abspath(TENSORBOARD_LOGS_PATH)}\\nor\\n\"\n",
    "      f\"tensorboard --logdir {path.abspath(current_tensorboard_log_dir)}\")\n",
    "\n",
    "best_weights_callback = SaveBestModelInMemory(metric=\"val_loss\")\n",
    "\n",
    "model.fit(x=train_data, y=train_labels, batch_size=BATCH_SIZE, epochs=750, validation_data=(test_data, test_labels), callbacks=[tensorboard_callback, best_weights_callback], verbose=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optional: Save model in memory"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved-models/film-shit/2022-12-17-18-48-29/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved-models/film-shit/2022-12-17-18-48-29/assets\n"
     ]
    }
   ],
   "source": [
    "model.set_weights(best_weights_callback.best_weights)\n",
    "saved_model_path = f\"{SAVED_MODELS_PATH}/{model_name}/{run_id}\"\n",
    "model.save(saved_model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optional: Create submission ZIP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created submission: ../submissions/film-shit/2022-12-17-18-48-29_no_feature_0.zip\n"
     ]
    }
   ],
   "source": [
    "submission_path = f\"{SUBMISSIONS_PATH}/{model_name}/{run_id}_no_feature_0\"\n",
    "create_submission_zip(submission_path, saved_model_path)\n",
    "\n",
    "print(f\"Created submission: {submission_path}.zip\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
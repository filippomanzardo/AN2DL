{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Title by Owner"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from os import path\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow.keras as keras\n",
    "from utils.callbacks import SaveBestModelInMemory\n",
    "from utils.submission import create_submission_zip"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Constants"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "NUM_CLASSES = 12\n",
    "RANDOM_STATE = 42 # Seed for rng to make everything reproducible and deterministic af\n",
    "SAVED_MODELS_PATH = \"saved-models\"\n",
    "TENSORBOARD_LOGS_PATH = \"tensorboard-logs\"\n",
    "SUBMISSIONS_PATH = \"../submissions\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128 # Number of samples in a mini batch\n",
    "EPOCHS = 100 # Number of training epochs before the training is stopped\n",
    "TEST_SPLIT = 0.15 # Percent of data to use for validation/testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Loading and Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All samples shape: (2429, 36, 6), all labels shape: (2429, 12)\n",
      "Train samples shape: (2064, 36, 6), Train labels shape: (2064, 12)\n",
      "Test samples shape: (365, 36, 6), Test labels shape: (365, 12)\n"
     ]
    }
   ],
   "source": [
    "data = np.load(file=\"../dataset/x_train.npy\")\n",
    "labels = tf.keras.utils.to_categorical(np.load(file=\"../dataset/y_train.npy\"), num_classes=NUM_CLASSES)\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=TEST_SPLIT, random_state=RANDOM_STATE)\n",
    "\n",
    "# Make sure everything was loaded correctly:\n",
    "print(f\"All samples shape: {data.shape}, all labels shape: {labels.shape}\")\n",
    "print(f\"Train samples shape: {train_data.shape}, Train labels shape: {train_labels.shape}\")\n",
    "print(f\"Test samples shape: {test_data.shape}, Test labels shape: {test_labels.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Definition"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "from utils.attention import Attention\n",
    "def build_model(self, input_shape: tuple[int,int,int], nb_classes: int) -> tf.keras.Model:\n",
    "        input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "        # data aug\n",
    "        data_aug = tf.keras.Sequential([\n",
    "            tf.keras.layers.GaussianNoise(0.5),\n",
    "            tf.keras.layers.GaussianDropout(0.5),\n",
    "        ])(input_layer)\n",
    "\n",
    "        concat = keras.layers.Concatenate(axis=-1)([input_layer, data_aug])\n",
    "\n",
    "        # conv block -1\n",
    "        conv_x = keras.layers.Conv1D(filters=64, kernel_size=8, padding='same')(concat)\n",
    "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "        conv_y = keras.layers.Conv1D(filters=32, kernel_size=5, padding='same')(conv_x)\n",
    "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "        conv_z = keras.layers.Conv1D(filters=32, kernel_size=3, padding='same')(conv_y)\n",
    "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "        # expand channels for the sum\n",
    "        shortcut_y = keras.layers.Conv1D(filters=32, kernel_size=1, padding='same')(input_layer)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "        output_block_1 = keras.layers.add([shortcut_y, conv_z])\n",
    "        output_block_1 = keras.layers.Activation('relu')(output_block_1)\n",
    "        # conv block -2\n",
    "        conv2 = keras.layers.Conv1D(filters=64,kernel_size=11,strides=1,padding='same')(output_block_1)\n",
    "        conv2 = tfa.layers.InstanceNormalization()(conv2)\n",
    "        conv2 = keras.layers.PReLU(shared_axes=[1])(conv2)\n",
    "        conv2 = keras.layers.Dropout(rate=0.3)(conv2)\n",
    "        conv2 = keras.layers.MaxPooling1D(pool_size=2)(conv2)\n",
    "        # conv block -3\n",
    "        conv3 = keras.layers.Conv1D(filters=128,kernel_size=21,strides=1,padding='same')(conv2)\n",
    "        conv3 = tfa.layers.InstanceNormalization()(conv3)\n",
    "        conv3 = keras.layers.PReLU(shared_axes=[1])(conv3)\n",
    "        conv3 = keras.layers.Dropout(rate=0.4)(conv3)\n",
    "        # expand channels for the sum\n",
    "        # split for attention\n",
    "        attention_data = keras.layers.Lambda(lambda x: x[:,:,:64])(conv3)\n",
    "        attention_softmax = keras.layers.Lambda(lambda x: x[:,:,64:])(conv3)\n",
    "        # attention mechanism\n",
    "        attention_softmax = keras.layers.Softmax()(attention_softmax)\n",
    "        multiply_layer = keras.layers.Multiply()([attention_softmax,attention_data])\n",
    "        attention_layer = Attention(units=16)(multiply_layer)\n",
    "        # last layer\n",
    "        dense_layer = keras.layers.Dense(units=8)(attention_layer)\n",
    "        act_layer = keras.layers.PReLU()(dense_layer)\n",
    "        dropout_layer = keras.layers.Dropout(rate=0.4)(act_layer)\n",
    "        # output layer\n",
    "        output_layer = keras.layers.Dense(units=nb_classes,activation='softmax')(dropout_layer)\n",
    "\n",
    "        return keras.models.Model(inputs=input_layer, outputs=output_layer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_42\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_56 (InputLayer)          [(None, 36, 6)]      0           []                               \n",
      "                                                                                                  \n",
      " sequential_47 (Sequential)     (None, 36, 6)        0           ['input_56[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 36, 12)       0           ['input_56[0][0]',               \n",
      "                                                                  'sequential_47[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_272 (Conv1D)            (None, 36, 64)       6208        ['concatenate_13[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_157 (Batch  (None, 36, 64)      256         ['conv1d_272[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_115 (Activation)    (None, 36, 64)       0           ['batch_normalization_157[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_273 (Conv1D)            (None, 36, 32)       10272       ['activation_115[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_158 (Batch  (None, 36, 32)      128         ['conv1d_273[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_116 (Activation)    (None, 36, 32)       0           ['batch_normalization_158[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_275 (Conv1D)            (None, 36, 32)       224         ['input_56[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_274 (Conv1D)            (None, 36, 32)       3104        ['activation_116[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_160 (Batch  (None, 36, 32)      128         ['conv1d_275[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_159 (Batch  (None, 36, 32)      128         ['conv1d_274[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_39 (Add)                   (None, 36, 32)       0           ['batch_normalization_160[0][0]',\n",
      "                                                                  'batch_normalization_159[0][0]']\n",
      "                                                                                                  \n",
      " activation_117 (Activation)    (None, 36, 32)       0           ['add_39[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_276 (Conv1D)            (None, 36, 64)       22592       ['activation_117[0][0]']         \n",
      "                                                                                                  \n",
      " instance_normalization_120 (In  (None, 36, 64)      128         ['conv1d_276[0][0]']             \n",
      " stanceNormalization)                                                                             \n",
      "                                                                                                  \n",
      " p_re_lu_154 (PReLU)            (None, 36, 64)       64          ['instance_normalization_120[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_154 (Dropout)          (None, 36, 64)       0           ['p_re_lu_154[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling1d_64 (MaxPooling1D  (None, 18, 64)      0           ['dropout_154[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv1d_277 (Conv1D)            (None, 18, 128)      172160      ['max_pooling1d_64[0][0]']       \n",
      "                                                                                                  \n",
      " instance_normalization_121 (In  (None, 18, 128)     256         ['conv1d_277[0][0]']             \n",
      " stanceNormalization)                                                                             \n",
      "                                                                                                  \n",
      " p_re_lu_155 (PReLU)            (None, 18, 128)      128         ['instance_normalization_121[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_155 (Dropout)          (None, 18, 128)      0           ['p_re_lu_155[0][0]']            \n",
      "                                                                                                  \n",
      " lambda_103 (Lambda)            (None, 18, 64)       0           ['dropout_155[0][0]']            \n",
      "                                                                                                  \n",
      " softmax_49 (Softmax)           (None, 18, 64)       0           ['lambda_103[0][0]']             \n",
      "                                                                                                  \n",
      " lambda_102 (Lambda)            (None, 18, 64)       0           ['dropout_155[0][0]']            \n",
      "                                                                                                  \n",
      " multiply_44 (Multiply)         (None, 18, 64)       0           ['softmax_49[0][0]',             \n",
      "                                                                  'lambda_102[0][0]']             \n",
      "                                                                                                  \n",
      " attention_23 (Attention)       (None, 16)           6144        ['multiply_44[0][0]']            \n",
      "                                                                                                  \n",
      " dense_84 (Dense)               (None, 8)            136         ['attention_23[0][0]']           \n",
      "                                                                                                  \n",
      " p_re_lu_156 (PReLU)            (None, 8)            8           ['dense_84[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_156 (Dropout)          (None, 8)            0           ['p_re_lu_156[0][0]']            \n",
      "                                                                                                  \n",
      " dense_85 (Dense)               (None, 12)           108         ['dropout_156[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 222,172\n",
      "Trainable params: 221,852\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n",
      "Run tensorboard in a separate process with:\n",
      "tensorboard --logdir /Users/filippomanzardo/repositories/AN2DL/challenge_2/notebooks/tensorboard-logs\n",
      "or\n",
      "tensorboard --logdir /Users/filippomanzardo/repositories/AN2DL/challenge_2/notebooks/tensorboard-logs/film-shit/2022-12-15-16-07-18\n",
      "Epoch 1/450\n",
      " 4/17 [======>.......................] - ETA: 0s - loss: 2.4702 - accuracy: 0.0938 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0218s vs `on_train_batch_end` time: 0.0378s). Check your callbacks.\n",
      "17/17 [==============================] - 3s 35ms/step - loss: 2.4470 - accuracy: 0.1860 - val_loss: 2.4079 - val_accuracy: 0.3315\n",
      "Epoch 2/450\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 2.3536 - accuracy: 0.2582 - val_loss: 2.2842 - val_accuracy: 0.3260\n",
      "Epoch 3/450\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 2.2638 - accuracy: 0.2917 - val_loss: 2.1605 - val_accuracy: 0.4219\n",
      "Epoch 4/450\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 2.1531 - accuracy: 0.3314 - val_loss: 2.0037 - val_accuracy: 0.4247\n",
      "Epoch 5/450\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 2.0920 - accuracy: 0.3532 - val_loss: 1.9241 - val_accuracy: 0.4164\n",
      "Epoch 6/450\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 2.0187 - accuracy: 0.3702 - val_loss: 1.8858 - val_accuracy: 0.4247\n",
      "Epoch 7/450\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 1.9700 - accuracy: 0.3832 - val_loss: 1.8534 - val_accuracy: 0.4055\n",
      "Epoch 8/450\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 1.9279 - accuracy: 0.3842 - val_loss: 1.8145 - val_accuracy: 0.4000\n",
      "Epoch 9/450\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 1.9312 - accuracy: 0.3915 - val_loss: 1.7740 - val_accuracy: 0.4192\n",
      "Epoch 10/450\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 1.8716 - accuracy: 0.3953 - val_loss: 1.7385 - val_accuracy: 0.4137\n",
      "Epoch 11/450\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 1.8390 - accuracy: 0.3983 - val_loss: 1.7065 - val_accuracy: 0.4274\n",
      "Epoch 12/450\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 1.8125 - accuracy: 0.4031 - val_loss: 1.6574 - val_accuracy: 0.4301\n",
      "Epoch 13/450\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 1.7998 - accuracy: 0.3978 - val_loss: 1.6583 - val_accuracy: 0.4356\n",
      "Epoch 14/450\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 1.7615 - accuracy: 0.4113 - val_loss: 1.6020 - val_accuracy: 0.4575\n",
      "Epoch 15/450\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 1.7307 - accuracy: 0.4210 - val_loss: 1.5966 - val_accuracy: 0.4411\n",
      "Epoch 16/450\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 1.7234 - accuracy: 0.4244 - val_loss: 1.6039 - val_accuracy: 0.4411\n",
      "Epoch 17/450\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 1.7076 - accuracy: 0.4331 - val_loss: 1.5466 - val_accuracy: 0.4603\n",
      "Epoch 18/450\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 1.6655 - accuracy: 0.4317 - val_loss: 1.5323 - val_accuracy: 0.4384\n",
      "Epoch 19/450\n",
      "17/17 [==============================] - 1s 33ms/step - loss: 1.6177 - accuracy: 0.4535 - val_loss: 1.5110 - val_accuracy: 0.4658\n",
      "Epoch 20/450\n",
      "17/17 [==============================] - 1s 35ms/step - loss: 1.6539 - accuracy: 0.4239 - val_loss: 1.5460 - val_accuracy: 0.4384\n",
      "Epoch 21/450\n",
      "17/17 [==============================] - 1s 33ms/step - loss: 1.6331 - accuracy: 0.4394 - val_loss: 1.4529 - val_accuracy: 0.4822\n",
      "Epoch 22/450\n",
      "17/17 [==============================] - 1s 32ms/step - loss: 1.6094 - accuracy: 0.4632 - val_loss: 1.4583 - val_accuracy: 0.4822\n",
      "Epoch 23/450\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 1.5678 - accuracy: 0.4641 - val_loss: 1.4466 - val_accuracy: 0.4685\n",
      "Epoch 24/450\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 1.5430 - accuracy: 0.4671 - val_loss: 1.4620 - val_accuracy: 0.4575\n",
      "Epoch 25/450\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 1.5456 - accuracy: 0.4656 - val_loss: 1.4179 - val_accuracy: 0.4959\n",
      "Epoch 26/450\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 1.5238 - accuracy: 0.4724 - val_loss: 1.4114 - val_accuracy: 0.4795\n",
      "Epoch 27/450\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 1.5214 - accuracy: 0.4767 - val_loss: 1.3646 - val_accuracy: 0.5425\n",
      "Epoch 28/450\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 1.4631 - accuracy: 0.4922 - val_loss: 1.3603 - val_accuracy: 0.5014\n",
      "Epoch 29/450\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 1.4736 - accuracy: 0.4845 - val_loss: 1.3485 - val_accuracy: 0.5178\n",
      "Epoch 30/450\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 1.4559 - accuracy: 0.4990 - val_loss: 1.2851 - val_accuracy: 0.5452\n",
      "Epoch 31/450\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 1.4395 - accuracy: 0.5039 - val_loss: 1.2766 - val_accuracy: 0.5425\n",
      "Epoch 32/450\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 1.4582 - accuracy: 0.4908 - val_loss: 1.2597 - val_accuracy: 0.5644\n",
      "Epoch 33/450\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 1.3685 - accuracy: 0.5270"
     ]
    }
   ],
   "source": [
    "input_shape = train_data.shape[1:]\n",
    "classes = NUM_CLASSES\n",
    "model_name = \"film-shit\" # Give your model an awesome name for a 2% percent accuracy increase.\n",
    "\n",
    "model = build_model(model_name, input_shape, classes)\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=keras.optimizers.Adam(3e-3),\n",
    "                      metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "run_id = datetime.utcnow().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "current_tensorboard_log_dir = f\"{TENSORBOARD_LOGS_PATH}/{model_name}/{run_id}\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=current_tensorboard_log_dir)\n",
    "print(f\"Run tensorboard in a separate process with:\\n\"\n",
    "      f\"tensorboard --logdir {path.abspath(TENSORBOARD_LOGS_PATH)}\\nor\\n\"\n",
    "      f\"tensorboard --logdir {path.abspath(current_tensorboard_log_dir)}\")\n",
    "\n",
    "best_weights_callback = SaveBestModelInMemory(metric=\"val_loss\")\n",
    "\n",
    "model.fit(x=train_data, y=train_labels, batch_size=BATCH_SIZE, epochs=450, validation_data=(test_data, test_labels), callbacks=[tensorboard_callback, best_weights_callback])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optional: Save model in memory"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved-models/My-awesome-model/2022-12-12-19-48-03/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved-models/My-awesome-model/2022-12-12-19-48-03/assets\n"
     ]
    }
   ],
   "source": [
    "model.set_weights(best_weights_callback.best_weights)\n",
    "saved_model_path = f\"{SAVED_MODELS_PATH}/{model_name}/{run_id}\"\n",
    "model.save(saved_model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optional: Create submission ZIP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created submission: ../submissions/My-awesome-model/2022-12-12-19-48-03.zip\n"
     ]
    }
   ],
   "source": [
    "submission_path = f\"{SUBMISSIONS_PATH}/{model_name}/{run_id}\"\n",
    "create_submission_zip(submission_path, saved_model_path)\n",
    "\n",
    "print(f\"Created submission: {submission_path}.zip\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}